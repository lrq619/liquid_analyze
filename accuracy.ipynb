{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from analyze import analyze_data\n",
    "from utils import RequestData\n",
    "import json\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import re\n",
    "# run_log_dir = \"/mnt/network_drive/lrq/logs/logs_2025-11-23-22-33-54/run_0\" # deepseek-ocr\n",
    "# run_log_dir = \"/mnt/network_drive/lrq/logs/logs_2025-11-23-23-48-36/run_0\" # llama3.1\n",
    "run_log_dir = \"/mnt/network_drive/lrq/logs/logs_2025-11-24-20-00-06/run_0\" # qwen\n",
    "dataset = \"/mnt/network_drive/lrq/traces/longBench/longbench_data/gt_qasper.json\"\n",
    "\n",
    "loadgen_result_file = f\"{run_log_dir}/loadgen_result.json\"\n",
    "loadgen_results = []\n",
    "with open(loadgen_result_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        loadgen_result = json.loads(line)\n",
    "        loadgen_results.append(loadgen_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "for loadgen_result in loadgen_results:\n",
    "    body = json.loads(loadgen_result['body'])\n",
    "    response = loadgen_result['response']['response']\n",
    "    # print(response)\n",
    "    if not response:\n",
    "        continue\n",
    "    if 'error' in response:\n",
    "        # print(f\"Error: {response['error']}\")\n",
    "        continue\n",
    "# response = json.loads(loadgen_results[0])\n",
    "    results_map[body['prompt']] = {\"prediction\": response['choices'][0]['text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize(prediction)\n",
    "    gt_tokens = normalize(ground_truth)\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Count overlapping tokens: intersection of multisets\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation Results =====\n",
      "Total Prompts Evaluated: 200\n",
      "\n",
      "Average F1 Score:      0.0516\n",
      "Average ROUGE-1 Score: 0.0578\n",
      "Average ROUGE-2 Score: 0.0316\n",
      "Average ROUGE-L Score: 0.0522\n",
      "\n",
      "=== Best by F1 ===\n",
      "F1 Score:    0.5574\n",
      "Prediction:  ####\n",
      "\n",
      "The languages explored in the study are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. These languages represent a diverse set of typologies, morphologies, and syntaxes, providing a comprehensive multilingual evaluation of the tagging models.\n",
      "Ground Truth:Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish\n",
      "\n",
      "=== Best by ROUGE-L ===\n",
      "ROUGE-L Score: 0.5574\n",
      "Prediction:    ####\n",
      "\n",
      "The languages explored in the study are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. These languages represent a diverse set of typologies, morphologies, and syntaxes, providing a comprehensive multilingual evaluation of the tagging models.\n",
      "Ground Truth:  Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# --- F1 helper functions ---\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = normalize(prediction)\n",
    "    gt_tokens = normalize(ground_truth)\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "# --- Load dataset ---\n",
    "with open(dataset, \"r\") as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "gt_answer_map = {item[\"prompt\"]: item[\"ground_truth_answer\"]\n",
    "                 for item in dataset_json}\n",
    "\n",
    "# ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "# --- Compute metrics ---\n",
    "for prompt in results_map:\n",
    "    prediction = results_map[prompt][\"prediction\"]\n",
    "    gt = gt_answer_map[prompt]\n",
    "\n",
    "    # F1\n",
    "    f1 = compute_f1(prediction, gt)\n",
    "    results_map[prompt][\"f1\"] = f1\n",
    "\n",
    "    # ROUGE\n",
    "    rs = scorer.score(gt, prediction)\n",
    "    results_map[prompt][\"rouge1\"] = rs[\"rouge1\"].fmeasure\n",
    "    results_map[prompt][\"rouge2\"] = rs[\"rouge2\"].fmeasure\n",
    "    results_map[prompt][\"rougeL\"] = rs[\"rougeL\"].fmeasure\n",
    "\n",
    "\n",
    "# --- Aggregate statistics ---\n",
    "all_items = list(results_map.items())\n",
    "\n",
    "avg_f1     = np.mean([v[\"f1\"]      for _, v in all_items])\n",
    "avg_r1     = np.mean([v[\"rouge1\"]  for _, v in all_items])\n",
    "avg_r2     = np.mean([v[\"rouge2\"]  for _, v in all_items])\n",
    "avg_rL     = np.mean([v[\"rougeL\"]  for _, v in all_items])\n",
    "\n",
    "# Best according to F1\n",
    "best_f1_prompt, best_f1_score = max(\n",
    "    [(p, v[\"f1\"]) for p, v in all_items],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "# Best according to ROUGE-L\n",
    "best_rL_prompt, best_rL_score = max(\n",
    "    [(p, v[\"rougeL\"]) for p, v in all_items],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"\\n===== Evaluation Results =====\")\n",
    "print(f\"Total Prompts Evaluated: {len(all_items)}\\n\")\n",
    "\n",
    "print(f\"Average F1 Score:      {avg_f1:.4f}\")\n",
    "print(f\"Average ROUGE-1 Score: {avg_r1:.4f}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_r2:.4f}\")\n",
    "print(f\"Average ROUGE-L Score: {avg_rL:.4f}\\n\")\n",
    "\n",
    "print(\"=== Best by F1 ===\")\n",
    "print(f\"F1 Score:    {best_f1_score:.4f}\")\n",
    "print(f\"Prediction:  {results_map[best_f1_prompt]['prediction']}\")\n",
    "print(f\"Ground Truth:{gt_answer_map[best_f1_prompt]}\\n\")\n",
    "\n",
    "print(\"=== Best by ROUGE-L ===\")\n",
    "print(f\"ROUGE-L Score: {best_rL_score:.4f}\")\n",
    "print(f\"Prediction:    {results_map[best_rL_prompt]['prediction']}\")\n",
    "print(f\"Ground Truth:  {gt_answer_map[best_rL_prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_qasper.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_multifieldqa_en.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_hotpotqa.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_2wikimqa.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_gov_report.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_multi_news.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_trec.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_triviaqa.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_samsum.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_passage_count.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_passage_retrieval_en.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_lcc.json\n",
      "Processing: /mnt/network_drive/lrq/traces/longBench/longbench_data/gt_repobench-p.json\n",
      "\n",
      "===== Results Across All Files =====\n",
      "Files processed: 13\n",
      "Total samples: 3150\n",
      "P99 token count: 680.5299999999993\n",
      "Max token count: 1147\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory containing your gt_*.json files\n",
    "DATA_DIR = \"/mnt/network_drive/lrq/traces/longBench/longbench_data\"\n",
    "\n",
    "# ---------- Token counter (word-based) ----------\n",
    "def count_word_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "all_token_counts = []\n",
    "files_processed = 0\n",
    "\n",
    "# Iterate over all JSON files starting with gt_\n",
    "for file_path in glob.glob(os.path.join(DATA_DIR, \"gt_*.json\")):\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    files_processed += 1\n",
    "\n",
    "    # Load dataset\n",
    "    with open(file_path, \"r\") as f:\n",
    "        dataset_json = json.load(f)\n",
    "\n",
    "    # Collect token counts for this file\n",
    "    for item in dataset_json:\n",
    "        gt_answer = item.get(\"ground_truth_answer\", \"\")\n",
    "        all_token_counts.append(count_word_tokens(gt_answer))\n",
    "\n",
    "print(\"\\n===== Results Across All Files =====\")\n",
    "print(\"Files processed:\", files_processed)\n",
    "print(\"Total samples:\", len(all_token_counts))\n",
    "\n",
    "# Compute P99 and max\n",
    "if len(all_token_counts) > 0:\n",
    "    p99 = np.percentile(all_token_counts, 99)\n",
    "    max_val = max(all_token_counts)\n",
    "    print(\"P99 token count:\", p99)\n",
    "    print(\"Max token count:\", max_val)\n",
    "else:\n",
    "    print(\"No token counts found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
